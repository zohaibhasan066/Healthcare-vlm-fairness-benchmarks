# -*- coding: utf-8 -*-
"""Open CLIP (ViT-H/14).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sJqcDYwk3wdW3fyrEP-7NHbo33GkZRwm

# **OpenAI CLIP (ViT-B/16)**
"""

# =============================================
# Streaming Preprocessing for FairFace using OpenCLIP ViT-H/14
# =============================================

import numpy as np
import pandas as pd
from pathlib import Path
from tqdm.auto import tqdm
from datasets import load_dataset
from transformers import CLIPProcessor, CLIPModel
import torch
import h5py  # Efficient on-disk storage for embeddings

# ---- Output paths ----
OUTPUT_DIR = Path("output_openclip_vith14")
OUTPUT_DIR.mkdir(exist_ok=True)
embeddings_path = OUTPUT_DIR / "fairface_clip_embeddings.h5"
metadata_path = OUTPUT_DIR / "fairface_metadata.csv"

# ---- Device and batch configuration ----
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 64   # Smaller batch due to ViT-H/14 memory requirements
EMB_DIM = 1024    # Dimension of ViT-H/14 image embeddings

# ---- CLIP Model Wrapper ----
class CLIP_Wrapper:
    """Wrapper for loading CLIP model and processing images to embeddings."""
    def __init__(self, model_name="laion/CLIP-ViT-H-14-laion2B-s32B-b79K", device="cuda"):
        self.device = device
        self.model = CLIPModel.from_pretrained(model_name).to(device)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()

    def encode_images(self, pil_images):
        """Convert a batch of PIL images to normalized CLIP embeddings."""
        inputs = self.processor(images=pil_images, return_tensors="pt").to(self.device)
        with torch.no_grad():
            feats = self.model.get_image_features(**inputs)
        return feats.cpu().numpy()

# ---- Normalize embeddings ----
def normalize(x, eps=1e-12):
    """L2 normalize embeddings row-wise."""
    norms = np.linalg.norm(x, axis=1, keepdims=True)
    return x / (norms + eps)

# ---- Streaming dataset and saving embeddings ----
def stream_and_save_embeddings(batch_size=BATCH_SIZE):
    """Load FairFace dataset, compute CLIP embeddings in batches, and save to HDF5."""

    # Load FairFace subsets
    ds_125 = load_dataset("HuggingFaceM4/FairFace", "1.25")
    ds_025 = load_dataset("HuggingFaceM4/FairFace", "0.25")

    # Combine all splits from both subsets
    datasets_all = [("1.25", split, ds_125[split]) for split in ds_125] + \
                   [("0.25", split, ds_025[split]) for split in ds_025]

    total_images = sum(len(d) for _, _, d in datasets_all)
    print(f"Total images combined: {total_images}")

    # Open HDF5 file for writing embeddings
    with h5py.File(embeddings_path, "w") as f:
        emb_dataset = f.create_dataset("image_embs", shape=(total_images, EMB_DIM), dtype=np.float32)
        model = CLIP_Wrapper(device=DEVICE)
        metadata_rows = []
        offset = 0

        # Process each split in batches
        for subset, split, ds in datasets_all:
            for i in tqdm(range(0, len(ds), batch_size), desc=f"Encoding {subset}-{split}"):
                batch = ds[i:i+batch_size]
                pil_imgs = batch["image"]

                # Compute embeddings and normalize
                emb = model.encode_images(pil_imgs)
                emb = normalize(emb)

                # Save embeddings to HDF5
                start = offset + i
                end = start + len(pil_imgs)
                emb_dataset[start:end, :] = emb

                # Collect metadata for each image
                for j in range(len(pil_imgs)):
                    metadata_rows.append({
                        "subset": subset,
                        "split": split,
                        "index_in_split": i + j,
                        "file_path": (batch.get("file", [None]*len(pil_imgs))[j]
                                      or batch.get("image_path", [None]*len(pil_imgs))[j]),
                        "age": batch.get("age", [None]*len(pil_imgs))[j],
                        "gender": batch.get("gender", [None]*len(pil_imgs))[j],
                        "race": batch.get("race", [None]*len(pil_imgs))[j],
                    })
            offset += len(ds)

    # Save metadata to CSV
    pd.DataFrame(metadata_rows).to_csv(metadata_path, index=False)
    print(f"Saved embeddings -> {embeddings_path}")
    print(f"Saved metadata  -> {metadata_path}")

# ---- Main function ----
def main():
    stream_and_save_embeddings()

if __name__ == "__main__":
    main()

# ==============================
# Run CLIP-based image retrieval for a specific profession
# ==============================

import numpy as np
import pandas as pd
from pathlib import Path
from transformers import CLIPProcessor, CLIPModel
import torch
from openpyxl import Workbook
from openpyxl.utils import get_column_letter
import h5py

# ---- File paths (update if using custom files) ----
embeddings_path = Path("/content/sample_data/fairface_clip_embeddings.h5")  # Precomputed image embeddings
metadata_path   = Path("/content/sample_data/fairface_metadata.csv")       # Metadata for images

DEVICE = "cuda:0"
TOP_K = 100  # Number of top matching images to retrieve

# ---- Set profession and output file ----
PROFESSION = "Medical Records Clerk"
PROMPT = f"Photo of a {PROFESSION}"
excel_output = Path(f"/content/{PROFESSION.replace(' ', '_')}.xlsx")


# ---- CLIP wrapper for text encoding ----
class OpenCLIP_Wrapper:
    def __init__(self, model_name="laion/CLIP-ViT-H-14-laion2B-s32B-b79K", device="cuda"):
        self.device = device
        self.model = CLIPModel.from_pretrained(model_name).to(device)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()

    def encode_texts(self, texts):
        """
        Encode a list of text prompts into CLIP text embeddings.
        """
        inputs = self.processor(text=texts, return_tensors="pt", padding=True).to(self.device)
        with torch.no_grad():
            text_emb = self.model.get_text_features(**inputs)
        return text_emb.cpu().numpy()


# ---- Save results to Excel ----
def save_to_excel(df, excel_path):
    """
    Save top-k retrieval results and metadata to an Excel file.
    """
    wb = Workbook()
    ws = wb.active
    ws.title = PROFESSION

    headers = ["Rank","Similarity","Subset","Split","Index","FilePath","Age","AgeGroup","Gender","Race"]
    for col_i, h in enumerate(headers, start=1):
        ws.cell(row=1, column=col_i, value=h)
        ws.column_dimensions[get_column_letter(col_i)].width = 20

    for row_ptr, (_, r) in enumerate(df.iterrows(), start=2):
        ws.cell(row=row_ptr, column=1, value=int(r["rank"]))
        ws.cell(row=row_ptr, column=2, value=float(r["similarity"]))
        ws.cell(row=row_ptr, column=3, value=str(r.get("subset", "")))
        ws.cell(row=row_ptr, column=4, value=str(r.get("split", "")))
        ws.cell(row=row_ptr, column=5, value=int(r.get("index_in_split", -1)))
        ws.cell(row=row_ptr, column=6, value=str(r.get("file_path", "")))
        ws.cell(row=row_ptr, column=7, value=str(r.get("age", "")))
        ws.cell(row=row_ptr, column=8, value=str(r.get("age_group", "")))
        ws.cell(row=row_ptr, column=9, value=str(r.get("gender", "")))
        ws.cell(row=row_ptr, column=10, value=str(r.get("race", "")))

    wb.save(excel_path)
    print(f"üìä Excel saved: {excel_path}")


# ---- Main processing ----
def main():
    # Load precomputed image embeddings
    with h5py.File(embeddings_path, "r") as f:
        image_embs = f["image_embs"][:]

    # Load metadata
    if metadata_path.suffix.lower() in [".xlsx", ".xls"]:
        metadata = pd.read_excel(metadata_path).fillna("")
    else:
        metadata = pd.read_csv(metadata_path).fillna("")
    records = metadata.to_dict("records")

    # Encode the text prompt using CLIP
    model = OpenCLIP_Wrapper(device=DEVICE)
    text_emb = model.encode_texts([PROMPT])[0]
    text_emb = text_emb / (np.linalg.norm(text_emb) + 1e-12)  # Normalize embedding

    # Compute cosine similarity between prompt and all image embeddings
    sims = image_embs.dot(text_emb)
    print(f"üîç Total images checked for '{PROFESSION}': {len(sims)}")

    # Retrieve top-K most similar images
    top_idx = np.argpartition(-sims, TOP_K-1)[:TOP_K]
    top_idx = top_idx[np.argsort(-sims[top_idx])]
    print(f"‚úÖ Top {TOP_K} images selected for '{PROFESSION}'")

    # Prepare top-K results
    rows = []
    for rank, idx in enumerate(top_idx, start=1):
        rec = records[idx]
        rec["rank"] = rank
        rec["similarity"] = float(sims[idx])
        rows.append(rec)
    df_top = pd.DataFrame(rows)

    # ---- Analyze age distribution ----
    print("\n--- RAW AGE DISTRIBUTION ---")
    print(df_top["age"].value_counts(sort=False, dropna=False))

    # Map raw ages into age groups
    def map_age_group(val):
        try:
            v = int(val)
        except:
            return "Unknown"
        if v in [0, 1, 2]:
            return "Young"
        elif v in [3, 4, 5]:
            return "Adult"
        elif v in [6, 7, 8]:
            return "Old"
        else:
            return "Unknown"

    df_top["age_group"] = df_top["age"].map(map_age_group)

    print("\n--- COLLAPSED AGE GROUPS ---")
    print(df_top["age_group"].value_counts(sort=False, dropna=False))

    print("\n--- GENDER DISTRIBUTION ---")
    print(df_top["gender"].value_counts(sort=False, dropna=False))

    print("\n--- RACE DISTRIBUTION ---")
    print(df_top["race"].value_counts(sort=False, dropna=False))

    # Save results to Excel
    save_to_excel(df_top, excel_output)


if __name__ == "__main__":
    main()