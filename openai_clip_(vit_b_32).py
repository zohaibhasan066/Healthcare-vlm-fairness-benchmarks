# -*- coding: utf-8 -*-
"""OpenAI CLIP (ViT-B/32).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sJqcDYwk3wdW3fyrEP-7NHbo33GkZRwm

# **OpenAI CLIP (ViT-B/16)**
"""

# =========================================================
# Streaming Preprocessing and Embedding Extraction for FairFace Dataset
# =========================================================

import numpy as np
import pandas as pd
from pathlib import Path
from tqdm.auto import tqdm
from datasets import load_dataset
from transformers import CLIPProcessor, CLIPModel
import torch
import h5py  # For efficient on-disk storage of embeddings

# -------------------- Paths --------------------
OUTPUT_DIR = Path("output_openai_clip_vitb32")
OUTPUT_DIR.mkdir(exist_ok=True)
embeddings_path = OUTPUT_DIR / "fairface_clip_embeddings.h5"
metadata_path = OUTPUT_DIR / "fairface_metadata.csv"

# -------------------- Device & Parameters --------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 128   # Suitable for A100 GPU
EMB_DIM = 512      # CLIP ViT-B/32 image embedding size

# -------------------- CLIP Wrapper --------------------
class CLIP_Wrapper:
    """Wrapper for CLIP model to simplify image feature extraction."""
    def __init__(self, model_name="openai/clip-vit-base-patch32", device="cuda"):
        self.device = device
        self.model = CLIPModel.from_pretrained(model_name).to(device)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()

    def encode_images(self, pil_images):
        """Convert a batch of PIL images to CLIP embeddings (numpy array)."""
        inputs = self.processor(images=pil_images, return_tensors="pt").to(self.device)
        with torch.no_grad():
            feats = self.model.get_image_features(**inputs)
        return feats.cpu().numpy()

# -------------------- Utility Functions --------------------
def normalize(x, eps=1e-12):
    """L2-normalize embeddings row-wise."""
    norms = np.linalg.norm(x, axis=1, keepdims=True)
    return x / (norms + eps)

# -------------------- Streaming & Saving --------------------
def stream_and_save_embeddings(batch_size=BATCH_SIZE):
    """Stream images from FairFace dataset, extract CLIP embeddings, and save them with metadata."""
    # Load FairFace subsets
    ds_125 = load_dataset("HuggingFaceM4/FairFace", "1.25")
    ds_025 = load_dataset("HuggingFaceM4/FairFace", "0.25")

    # Combine all splits
    datasets_all = [("1.25", split, ds_125[split]) for split in ds_125] + \
                   [("0.25", split, ds_025[split]) for split in ds_025]

    # Total number of images
    total_images = sum(len(d) for _, _, d in datasets_all)
    print(f"Total images combined: {total_images}")

    # Initialize HDF5 file for embeddings
    with h5py.File(embeddings_path, "w") as f:
        emb_dataset = f.create_dataset("image_embs", shape=(total_images, EMB_DIM), dtype=np.float32)

        model = CLIP_Wrapper(device=DEVICE)
        metadata_rows = []
        offset = 0

        # Process each dataset split in batches
        for subset, split, ds in datasets_all:
            for i in tqdm(range(0, len(ds), batch_size), desc=f"Encoding {subset}-{split}"):
                batch = ds[i:i+batch_size]
                pil_imgs = batch["image"]

                # Extract and normalize embeddings
                emb = model.encode_images(pil_imgs)
                emb = normalize(emb)

                # Store embeddings in HDF5
                start = offset + i
                end = start + len(pil_imgs)
                emb_dataset[start:end, :] = emb

                # Collect metadata
                for j in range(len(pil_imgs)):
                    metadata_rows.append({
                        "subset": subset,
                        "split": split,
                        "index_in_split": i + j,
                        "file_path": (batch.get("file", [None]*len(pil_imgs))[j]
                                      or batch.get("image_path", [None]*len(pil_imgs))[j]),
                        "age": batch.get("age", [None]*len(pil_imgs))[j],
                        "gender": batch.get("gender", [None]*len(pil_imgs))[j],
                        "race": batch.get("race", [None]*len(pil_imgs))[j],
                    })

            offset += len(ds)

    # Save metadata to CSV
    pd.DataFrame(metadata_rows).to_csv(metadata_path, index=False)

    print(f"Saved embeddings -> {embeddings_path}")
    print(f"Saved metadata  -> {metadata_path}")

# -------------------- Main --------------------
def main():
    stream_and_save_embeddings()

if __name__ == "__main__":
    main()

# ==============================
# PART 2: Run CLIP similarity search per profession
# Using ViT-B/32 with preloaded embeddings and metadata
# ==============================

import numpy as np
import pandas as pd
from pathlib import Path
from transformers import CLIPProcessor, CLIPModel
import torch
from openpyxl import Workbook
from openpyxl.utils import get_column_letter
import h5py

# ---- File paths (update to your data locations) ----
embeddings_path = Path("/content/sample_data/fairface_clip_embeddings.h5")
metadata_path   = Path("/content/sample_data/fairface_metadata.csv")

# ---- Device and parameters ----
DEVICE = "cuda:0"
TOP_K = 100  # Number of top matches to retrieve

# ---- Profession search ----
PROFESSION = "Medical Records Clerk"
PROMPT = f"Photo of a {PROFESSION}"
excel_output = Path(f"/content/{PROFESSION.replace(' ', '_')}.xlsx")

# ---- CLIP wrapper for text encoding ----
class OpenAI_CLIP_Wrapper:
    def __init__(self, model_name="openai/clip-vit-base-patch32", device="cuda"):
        self.device = device
        self.model = CLIPModel.from_pretrained(model_name).to(device)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()

    def encode_texts(self, texts):
        """
        Encode a list of text prompts into CLIP embeddings.
        """
        inputs = self.processor(text=texts, return_tensors="pt", padding=True).to(self.device)
        with torch.no_grad():
            text_emb = self.model.get_text_features(**inputs)
        return text_emb.cpu().numpy()

# ---- Function to save results to Excel ----
def save_to_excel(df, excel_path):
    """
    Save metadata and similarity results to an Excel file.
    """
    wb = Workbook()
    ws = wb.active
    ws.title = PROFESSION

    headers = ["Rank","Similarity","Subset","Split","Index","FilePath","Age","AgeGroup","Gender","Race"]
    for col_i, h in enumerate(headers, start=1):
        ws.cell(row=1, column=col_i, value=h)
        ws.column_dimensions[get_column_letter(col_i)].width = 20

    for row_ptr, (_, r) in enumerate(df.iterrows(), start=2):
        ws.cell(row=row_ptr, column=1, value=int(r["rank"]))
        ws.cell(row=row_ptr, column=2, value=float(r["similarity"]))
        ws.cell(row=row_ptr, column=3, value=str(r.get("subset", "")))
        ws.cell(row=row_ptr, column=4, value=str(r.get("split", "")))
        ws.cell(row=row_ptr, column=5, value=int(r.get("index_in_split", -1)))
        ws.cell(row=row_ptr, column=6, value=str(r.get("file_path", "")))
        ws.cell(row=row_ptr, column=7, value=str(r.get("age", "")))
        ws.cell(row=row_ptr, column=8, value=str(r.get("age_group", "")))
        ws.cell(row=row_ptr, column=9, value=str(r.get("gender", "")))
        ws.cell(row=row_ptr, column=10, value=str(r.get("race", "")))

    wb.save(excel_path)
    print(f"üìä Excel saved: {excel_path}")

# ---- Main search function ----
def main():
    # Load precomputed image embeddings
    with h5py.File(embeddings_path, "r") as f:
        image_embs = f["image_embs"][:]

    # Load metadata
    if metadata_path.suffix.lower() in [".xlsx", ".xls"]:
        metadata = pd.read_excel(metadata_path).fillna("")
    else:
        metadata = pd.read_csv(metadata_path).fillna("")
    records = metadata.to_dict("records")

    # Encode profession prompt
    model = OpenAI_CLIP_Wrapper(device=DEVICE)
    text_emb = model.encode_texts([PROMPT])[0]
    text_emb = text_emb / (np.linalg.norm(text_emb) + 1e-12)  # Normalize embedding

    # Compute similarity between prompt and image embeddings
    sims = image_embs.dot(text_emb)
    print(f"üîç Total images checked for '{PROFESSION}': {len(sims)}")

    # Select top-K images
    top_idx = np.argpartition(-sims, TOP_K-1)[:TOP_K]
    top_idx = top_idx[np.argsort(-sims[top_idx])]
    print(f"‚úÖ Top {TOP_K} images selected for '{PROFESSION}'")

    # Collect results into DataFrame
    rows = []
    for rank, idx in enumerate(top_idx, start=1):
        rec = records[idx]
        rec["rank"] = rank
        rec["similarity"] = float(sims[idx])
        rows.append(rec)
    df_top = pd.DataFrame(rows)

    # ---- Age group mapping ----
    print("\n--- RAW AGE DISTRIBUTION ---")
    print(df_top["age"].value_counts(sort=False, dropna=False))

    def map_age_group(val):
        """
        Collapse numeric age codes into 'Young', 'Adult', 'Old', or 'Unknown'.
        """
        try:
            v = int(val)
        except:
            return "Unknown"
        if v in [0, 1, 2]:
            return "Young"
        elif v in [3, 4, 5]:
            return "Adult"
        elif v in [6, 7, 8]:
            return "Old"
        else:
            return "Unknown"

    df_top["age_group"] = df_top["age"].map(map_age_group)

    print("\n--- COLLAPSED AGE GROUPS ---")
    print(df_top["age_group"].value_counts(sort=False, dropna=False))

    # ---- Gender and race distribution ----
    print("\n--- GENDER DISTRIBUTION ---")
    print(df_top["gender"].value_counts(sort=False, dropna=False))

    print("\n--- RACE DISTRIBUTION ---")
    print(df_top["race"].value_counts(sort=False, dropna=False))

    # Save results to Excel
    save_to_excel(df_top, excel_output)

if __name__ == "__main__":
    main()